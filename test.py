"""
æ®‹å·®åˆ†å¸ƒåˆ†æä¸å¯è§†åŒ–å·¥å…·
åŠŸèƒ½ï¼š
1. è®­ç»ƒæ¨¡å‹
2. æå–é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾
3. è®¡ç®—æ®‹å·®
4. æ‹Ÿåˆå¤šç§åˆ†å¸ƒå¹¶é€‰æ‹©æœ€ä¼˜
5. å¯è§†åŒ–å¯¹æ¯”
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.optimize import minimize
import json
from pathlib import Path
from typing import Dict, List, Tuple, Callable
import warnings
warnings.filterwarnings('ignore')

# ============================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ”¯æŒçš„åˆ†å¸ƒç±»å‹
# ============================================

class DistributionFitter:
    """
    æ”¯æŒå¤šç§æ¦‚ç‡åˆ†å¸ƒçš„æ‹Ÿåˆä¸è¯Šæ–­
    """
    
    DISTRIBUTIONS = {
        'normal': {
            'scipy': stats.norm,
            'params': ['loc', 'scale'],
            'description': 'Normal (Gaussian) Distribution'
        },
        'beta': {
            'scipy': stats.beta,
            'params':  ['a', 'b', 'loc', 'scale'],
            'description': 'Beta Distribution (é€‚åˆ [0,1] åŒºé—´)'
        },
        'gamma':  {
            'scipy': stats. gamma,
            'params': ['a', 'loc', 'scale'],
            'description': 'Gamma Distribution (é€‚åˆæ­£ååˆ†å¸ƒ)'
        },
        'lognorm': {
            'scipy': stats.lognorm,
            'params': ['s', 'loc', 'scale'],
            'description': 'Log-Normal Distribution'
        },
        'weibull': {
            'scipy': stats.weibull_min,
            'params': ['c', 'loc', 'scale'],
            'description': 'Weibull Distribution'
        },
        'laplace': {
            'scipy': stats.laplace,
            'params': ['loc', 'scale'],
            'description': 'Laplace Distribution (åŒæŒ‡æ•°åˆ†å¸ƒï¼Œé€‚åˆæœ‰å°–å³°)'
        },
        't': {
            'scipy': stats.t,
            'params':  ['df', 'loc', 'scale'],
            'description':  'Student-t Distribution (é‡å°¾åˆ†å¸ƒ)'
        },
        'cauchy': {
            'scipy':  stats.cauchy,
            'params': ['loc', 'scale'],
            'description': 'Cauchy Distribution (æé‡å°¾)'
        },
        'bernouÂ·Â·Â·Â·Â·Â·Â·Â·Â·lli': {
            'scipy': stats.bernoulli,
            'params': ['p', 'loc'],
            'description': 'Bernoulli Distribution (0/1 äºŒå…ƒåˆ†å¸ƒï¼Œç¦»æ•£)'
        }
    }
    
    @staticmethod
    def fit_distribution(data: np.ndarray, dist_name: str) -> Tuple[tuple, float, float]:
        """
        æ‹ŸåˆæŒ‡å®šåˆ†å¸ƒ
        
        Returns:
            params: æ‹Ÿåˆçš„å‚æ•°
            aic: èµ¤æ± ä¿¡æ¯å‡†åˆ™ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
            bic: è´å¶æ–¯ä¿¡æ¯å‡†åˆ™ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        """
        dist = DistributionFitter. DISTRIBUTIONS[dist_name]['scipy']
        
        # æ‹Ÿåˆå‚æ•°
        try:
            params = dist.fit(data)
        except Exception as e:
            print(f"è­¦å‘Š:  {dist_name} æ‹Ÿåˆå¤±è´¥ - {e}")
            return None, np.inf, np.inf
        
        # è®¡ç®—å¯¹æ•°ä¼¼ç„¶
        log_likelihood = np.sum(dist.logpdf(data, *params))
        
        # è®¡ç®— AIC å’Œ BIC
        k = len(params)  # å‚æ•°ä¸ªæ•°
        n = len(data)
        aic = 2 * k - 2 * log_likelihood
        bic = k * np.log(n) - 2 * log_likelihood
        
        return params, aic, bic
    
    @staticmethod
    def goodness_of_fit_tests(data: np.ndarray, dist_name: str, params: tuple) -> Dict:
        """
        æ‰§è¡Œæ‹Ÿåˆä¼˜åº¦æ£€éªŒ
        """
        dist = DistributionFitter. DISTRIBUTIONS[dist_name]['scipy']
        
        # 1.  Kolmogorov-Smirnov æ£€éªŒ
        ks_stat, ks_p = stats.kstest(data, lambda x: dist.cdf(x, *params))
        
        # 2. Anderson-Darling æ£€éªŒï¼ˆä»…æ”¯æŒéƒ¨åˆ†åˆ†å¸ƒï¼‰
        ad_stat, ad_crit, ad_sig = None, None, None
        if dist_name in ['normal', 'lognorm']: 
            try:
                ad_result = stats.anderson(data, dist=dist_name)
                ad_stat = ad_result.statistic
                ad_crit = ad_result.critical_values
                ad_sig = ad_result.significance_level
            except:
                pass
        
        return {
            'ks_statistic': ks_stat,
            'ks_p_value': ks_p,
            'ad_statistic': ad_stat,
            'ad_critical_values': ad_crit,
            'ad_significance_level': ad_sig
        }

# ============================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ®‹å·®æå–å™¨
# ============================================

class ResidualExtractor:
    """
    ä»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­æå–æ®‹å·®
    """
    
    def __init__(self, model:  tf.keras.Model, dataset: tf.data.Dataset):
        self.model = model
        self.dataset = dataset
        self.residuals = {}
        self.predictions = {}
        self.labels = {}
    
    def extract(self, num_batches: int = None) -> Dict[str, np.ndarray]: 
        """
        æå–æ®‹å·®ï¼šresidual = label - prediction
        
        å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼š
        - label âˆˆ {0, 1}
        - prediction âˆˆ [0, 1] (sigmoid è¾“å‡º)
        - residual âˆˆ [-1, 1]
        """
        all_predictions = {key: [] for key in ['paid_treatment_0', 'paid_treatment_1',
                                                 'cost_treatment_0', 'cost_treatment_1']}
        all_labels = {'paid':  [], 'cost': [], 'treatment': []}
        
        for batch_idx, (features, labels) in enumerate(self.dataset):
            if num_batches and batch_idx >= num_batches:
                break
            
            # å‰å‘ä¼ æ’­
            predictions = self.model(features, training=False)
            
            # æ”¶é›†é¢„æµ‹å€¼ï¼ˆlogit -> probabilityï¼‰
            for key in all_predictions. keys():
                if key in predictions:
                    logits = predictions[key]. numpy()
                    probs = 1 / (1 + np.exp(-np.clip(logits, -10, 10)))  # sigmoid
                    all_predictions[key].append(probs)
            
            # æ”¶é›†æ ‡ç­¾
            all_labels['paid'].append(labels['paid'].numpy())
            all_labels['cost'].append(labels['cost'].numpy())
            all_labels['treatment'].append(labels['treatment'].numpy())
        
        # åˆå¹¶æ‰€æœ‰ batch
        for key in all_predictions: 
            all_predictions[key] = np.concatenate(all_predictions[key])
        
        for key in all_labels:
            all_labels[key] = np.concatenate(all_labels[key])
        
        # è®¡ç®—æ®‹å·®ï¼ˆæŒ‰ treatment åˆ†ç»„ï¼‰
        residuals = {}
        treatment_idx = all_labels['treatment'].astype(int)
        
        for target in ['paid', 'cost']: 
            for treatment in [0, 1]: 
                mask = (treatment_idx == treatment)
                pred_key = f"{target}_treatment_{treatment}"
                
                y_true = all_labels[target][mask]
                y_pred = all_predictions[pred_key][mask]
                
                # æ®‹å·® = çœŸå®å€¼ - é¢„æµ‹å€¼
                residual = y_true - y_pred
                
                residuals[pred_key] = residual
                self.predictions[pred_key] = y_pred
                self.labels[f"{target}_{treatment}"] = y_true
        
        self.residuals = residuals
        return residuals

# ============================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¯è§†åŒ–å·¥å…·
# ============================================

class ResidualVisualizer:
    """
    æ®‹å·®åˆ†å¸ƒçš„å¯è§†åŒ–å·¥å…·
    """
    
    @staticmethod
    def plot_residual_analysis(residuals: Dict[str, np.ndarray], 
                                fitted_distributions: Dict,
                                output_dir: str = './residual_analysis'):
        """
        ç”Ÿæˆå®Œæ•´çš„æ®‹å·®åˆ†ææŠ¥å‘Š
        """
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        for res_key, res_data in residuals.items():
            # æ¯ä¸ªæ®‹å·®ç±»å‹ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„å›¾
            fig = plt.figure(figsize=(20, 12))
            gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)
            
            # 1. åŸå§‹æ®‹å·®ç›´æ–¹å›¾ + KDE
            ax1 = fig.add_subplot(gs[0, : 2])
            sns.histplot(res_data, bins=50, kde=True, stat='density', ax=ax1, alpha=0.6)
            ax1.set_title(f'{res_key} - Residual Distribution', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Residual (True - Predicted)')
            ax1.set_ylabel('Density')
            ax1.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Residual')
            ax1.legend()
            
            # 2. Q-Q å›¾ï¼ˆä¸æ­£æ€åˆ†å¸ƒå¯¹æ¯”ï¼‰
            ax2 = fig.add_subplot(gs[0, 2:])
            stats.probplot(res_data, dist="norm", plot=ax2)
            ax2.set_title(f'{res_key} - Q-Q Plot (vs Normal)', fontsize=14)
            
            # 3-6. æ‹Ÿåˆçš„åˆ†å¸ƒå¯¹æ¯”
            best_dists = fitted_distributions[res_key]['ranked_distributions'][: 4]
            
            for idx, (dist_name, dist_info) in enumerate(best_dists):
                row = (idx // 2) + 1
                col = (idx % 2) * 2
                ax = fig.add_subplot(gs[row, col: col+2])
                
                # ç»˜åˆ¶ç›´æ–¹å›¾
                ax.hist(res_data, bins=50, density=True, alpha=0.5, 
                       edgecolor='black', label='Observed')
                
                # ç»˜åˆ¶æ‹Ÿåˆçš„åˆ†å¸ƒ
                dist = DistributionFitter. DISTRIBUTIONS[dist_name]['scipy']
                params = dist_info['params']
                x = np.linspace(res_data.min(), res_data.max(), 200)
                pdf = dist.pdf(x, *params)
                ax.plot(x, pdf, 'r-', linewidth=2, 
                       label=f'{dist_name. capitalize()} Fit')
                
                # æ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯
                textstr = (f"AIC: {dist_info['aic']:.2f}\n"
                          f"BIC: {dist_info['bic']:.2f}\n"
                          f"KS p-value: {dist_info['ks_p_value']:.4f}")
                ax.text(0.65, 0.95, textstr, transform=ax. transAxes,
                       verticalalignment='top', bbox=dict(boxstyle='round', 
                       facecolor='wheat', alpha=0.5), fontsize=10)
                
                ax.set_title(f'{dist_name.capitalize()} Distribution', fontsize=12)
                ax.set_xlabel('Residual')
                ax.set_ylabel('Density')
                ax.legend()
                ax.grid(alpha=0.3)
            
            plt.savefig(f'{output_dir}/{res_key}_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… å·²ç”Ÿæˆ {res_key} çš„åˆ†æå›¾è¡¨")
    
    @staticmethod
    def generate_summary_report(fitted_distributions: Dict, output_path: str):
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦æŠ¥å‘Š
        """
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("æ®‹å·®åˆ†å¸ƒæ‹ŸåˆæŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            for res_key, dist_results in fitted_distributions.items():
                f.write(f"\n{'='*60}\n")
                f.write(f"ç›®æ ‡:  {res_key}\n")
                f.write(f"{'='*60}\n\n")
                
                ranked = dist_results['ranked_distributions']
                
                f.write("åˆ†å¸ƒæ‹Ÿåˆæ’å (æŒ‰ AIC ä»å°åˆ°å¤§):\n\n")
                
                for rank, (dist_name, info) in enumerate(ranked, 1):
                    f.write(f"{rank}. {dist_name. upper()}\n")
                    f.write(f"   æè¿°: {DistributionFitter.DISTRIBUTIONS[dist_name]['description']}\n")
                    f.write(f"   AIC: {info['aic']:.4f}\n")
                    f.write(f"   BIC: {info['bic']:.4f}\n")
                    f.write(f"   KSç»Ÿè®¡é‡: {info['ks_statistic']:.4f}\n")
                    f.write(f"   KS på€¼: {info['ks_p_value']:.4f}\n")
                    f.write(f"   å‚æ•°:  {info['params']}\n")
                    
                    # åˆ¤æ–­æ˜¯å¦æ¥å—è¯¥åˆ†å¸ƒï¼ˆp > 0.05 è¡¨ç¤ºä¸èƒ½æ‹’ç»åŸå‡è®¾ï¼‰
                    if info['ks_p_value'] > 0.05:
                        f.write(f"   âœ… æ‹Ÿåˆä¼˜åº¦æ£€éªŒé€šè¿‡ (p > 0.05)\n")
                    else: 
                        f.write(f"   âŒ æ‹Ÿåˆä¼˜åº¦æ£€éªŒæœªé€šè¿‡ (p < 0.05)\n")
                    
                    f.write("\n")
                
                # æ¨è
                best_dist_name, best_dist_info = ranked[0]
                f.write(f"\nğŸ¯ æ¨èåˆ†å¸ƒ: {best_dist_name.upper()}\n")
                f.write(f"   åŸå› : AIC æœ€å° ({best_dist_info['aic']:.4f})\n\n")
        
        print(f"âœ… æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")

# ============================================
# ç¬¬å››éƒ¨åˆ†ï¼šè‡ªå®šä¹‰æŸå¤±å‡½æ•°ç”Ÿæˆå™¨
# ============================================

class CustomLossGenerator:
    """
    æ ¹æ®æ‹Ÿåˆçš„åˆ†å¸ƒè‡ªåŠ¨ç”Ÿæˆå¯¹åº”çš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±å‡½æ•°
    """
    
    @staticmethod
    def generate_nll_loss(dist_name: str, fitted_params: tuple) -> Callable:
        """
        ç”ŸæˆåŸºäºç‰¹å®šåˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆTensorFlowç‰ˆæœ¬ï¼‰
        
        æ³¨æ„ï¼šå¯¹äºå›å½’é—®é¢˜ï¼Œæ®‹å·® = y_true - y_pred
        æˆ‘ä»¬è¦å»ºæ¨¡æ®‹å·®çš„åˆ†å¸ƒï¼Œå› æ­¤æŸå¤±æ˜¯ -log P(residual | params)
        """
        
        if dist_name == 'normal': 
            # æ­£æ€åˆ†å¸ƒ:  N(0, ÏƒÂ²)
            # å‚æ•°: loc=0, scale=Ïƒ
            loc, scale = fitted_params
            
            def normal_nll(y_true, y_pred):
                """
                y_true: çœŸå®æ ‡ç­¾ [batch_size]
                y_pred: é¢„æµ‹å€¼ [batch_size]
                """
                residual = y_true - y_pred
                # -log p(residual) = 0.5 * log(2Ï€ÏƒÂ²) + (residual - Î¼)Â² / (2ÏƒÂ²)
                nll = 0.5 * tf.math.log(2 * np.pi * scale**2) + \
                      0.5 * tf.square(residual - loc) / (scale**2)
                return tf.reduce_mean(nll)
            
            return normal_nll
        
        elif dist_name == 'laplace':
            # æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ (L1æŸå¤±çš„æ¦‚ç‡è§£é‡Š)
            loc, scale = fitted_params
            
            def laplace_nll(y_true, y_pred):
                residual = y_true - y_pred
                nll = tf.math.log(2 * scale) + tf.abs(residual - loc) / scale
                return tf.reduce_mean(nll)
            
            return laplace_nll
        
        elif dist_name == 't':
            # Student-t åˆ†å¸ƒï¼ˆå¯¹ç¦»ç¾¤å€¼æ›´é²æ£’ï¼‰
            df, loc, scale = fitted_params
            
            def t_nll(y_true, y_pred):
                residual = y_true - y_pred
                # tåˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆè¿‘ä¼¼ï¼‰
                standardized = (residual - loc) / scale
                nll = -tf.math.lgamma((df + 1) / 2) + tf.math.lgamma(df / 2) + \
                      0.5 * tf.math. log(df * np.pi * scale**2) + \
                      ((df + 1) / 2) * tf.math.log(1 + tf.square(standardized) / df)
                return tf. reduce_mean(nll)
            
            return t_nll
        
        elif dist_name == 'beta':
            # Beta åˆ†å¸ƒï¼ˆé€‚åˆ [0, 1] èŒƒå›´çš„æ®‹å·®ï¼‰
            a, b, loc, scale = fitted_params
            
            def beta_nll(y_true, y_pred):
                residual = y_true - y_pred
                # å°†æ®‹å·®æ˜ å°„åˆ° [0, 1]
                residual_scaled = (residual - loc) / scale
                residual_clipped = tf.clip_by_value(residual_scaled, 1e-6, 1 - 1e-6)
                
                # Beta åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶
                nll = -((a - 1) * tf.math.log(residual_clipped) + \
                       (b - 1) * tf.math.log(1 - residual_clipped)) + \
                      tf.math. lgamma(a) + tf.math.lgamma(b) - tf.math.lgamma(a + b)
                return tf.reduce_mean(nll)
            
            return beta_nll
        
        elif dist_name == 'gamma':
            # Gamma åˆ†å¸ƒ
            a, loc, scale = fitted_params
            
            def gamma_nll(y_true, y_pred):
                residual = y_true - y_pred
                # Gamma è¦æ±‚ x > 0ï¼Œéœ€è¦å¹³ç§»
                residual_shifted = residual - loc
                residual_clipped = tf.maximum(residual_shifted, 1e-6)
                
                nll = a * tf.math.log(scale) + tf.math.lgamma(a) - \
                      (a - 1) * tf.math.log(residual_clipped) + \
                      residual_clipped / scale
                return tf.reduce_mean(nll)
            
            return gamma_nll

        elif dist_name == 'bernoulli':
            # ä¼¯åŠªåˆ©åˆ†å¸ƒ: å¯¹åº”äºŒåˆ†ç±»é—®é¢˜çš„ Binary Cross Entropy
            # æ³¨æ„ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒä¸ç”¨äºå»ºæ¨¡â€œæ®‹å·®â€(y - y_pred)ï¼Œè€Œæ˜¯ç›´æ¥å»ºæ¨¡ y_true (0æˆ–1)
            # fitted_params åœ¨è¿™é‡Œé€šå¸¸æ˜¯å…¨å±€çš„å¹³å‡æ¦‚ç‡ pï¼Œä½†åœ¨ NLL è®¡ç®—ä¸­æˆ‘ä»¬é€šå¸¸ä½¿ç”¨æ¨¡å‹è¾“å‡ºçš„ y_pred ä½œä¸ºåŠ¨æ€æ¦‚ç‡
            
            # scipy.stats.bernoulli.fit é€šå¸¸è¿”å› (p, loc)ï¼Œè¿™é‡Œä¸»è¦ä¸ºäº†è§£åŒ…å‚æ•°å ä½
            if len(fitted_params) >= 1:
                p_prior = fitted_params[0] 
            
            def bernoulli_nll(y_true, y_pred):
                """
                y_true: çœŸå®æ ‡ç­¾ï¼Œå¿…é¡»æ˜¯ 0 æˆ– 1
                y_pred: æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡å€¼ (é€šå¸¸ç»è¿‡ Sigmoid)ï¼ŒèŒƒå›´ (0, 1)
                """
                # æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼šé˜²æ­¢ log(0)
                # å°†æ¦‚ç‡é™åˆ¶åœ¨ [epsilon, 1-epsilon] ä¹‹é—´
                epsilon = 1e-7
                y_pred_clipped = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)
                
                # ä¼¯åŠªåˆ©åˆ†å¸ƒçš„è´Ÿå¯¹æ•°ä¼¼ç„¶å…¬å¼ (å³äºŒå…ƒäº¤å‰ç†µ):
                # NLL = - [y * log(p) + (1-y) * log(1-p)]
                nll = -(y_true * tf.math.log(y_pred_clipped) + \
                        (1 - y_true) * tf.math.log(1 - y_pred_clipped))
                
                return tf.reduce_mean(nll)
            
            return bernoulli_nll
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å¸ƒ:  {dist_name}")
    
    @staticmethod
    def save_loss_functions_as_code(fitted_distributions: Dict, output_path: str):
        """
        å°†æœ€ä¼˜æŸå¤±å‡½æ•°ä¿å­˜ä¸ºå¯ç›´æ¥ä½¿ç”¨çš„ Python ä»£ç 
        """
        code_lines = [
            "# è‡ªåŠ¨ç”Ÿæˆçš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°\n",
            "import tensorflow as tf\n",
            "import numpy as np\n\n"
        ]
        
        for res_key, dist_results in fitted_distributions.items():
            best_dist_name, best_dist_info = dist_results['ranked_distributions'][0]
            params = best_dist_info['params']
            
            code_lines.append(f"# {res_key} çš„æœ€ä¼˜æŸå¤±å‡½æ•° (åŸºäº {best_dist_name} åˆ†å¸ƒ)\n")
            code_lines.append(f"# AIC: {best_dist_info['aic']:.4f}, BIC: {best_dist_info['bic']:.4f}\n\n")
            
            # ç”Ÿæˆå‡½æ•°ä»£ç 
            func_name = f"loss_{res_key. replace('_', '__')}"
            
            if best_dist_name == 'normal':
                loc, scale = params
                code_lines.append(f"def {func_name}(y_true, y_pred):\n")
                code_lines.append(f"    \"\"\"æ­£æ€åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (Î¼={loc:.4f}, Ïƒ={scale:.4f})\"\"\"\n")
                code_lines.append(f"    residual = y_true - y_pred\n")
                code_lines.append(f"    nll = 0.5 * tf.math.log({2 * np.pi * scale**2:. 6f}) + \\\n")
                code_lines. append(f"          0.5 * tf.square(residual - {loc:.6f}) / {scale**2:.6f}\n")
                code_lines.append(f"    return tf.reduce_mean(nll)\n\n")
            
            elif best_dist_name == 'laplace':
                loc, scale = params
                code_lines.append(f"def {func_name}(y_true, y_pred):\n")
                code_lines.append(f"    \"\"\"æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (Î¼={loc:.4f}, b={scale:.4f})\"\"\"\n")
                code_lines.append(f"    residual = y_true - y_pred\n")
                code_lines.append(f"    nll = tf.math.log({2 * scale:.6f}) + tf.abs(residual - {loc:.6f}) / {scale:.6f}\n")
                code_lines.append(f"    return tf.reduce_mean(nll)\n\n")
            
            elif best_dist_name == 't':
                df, loc, scale = params
                code_lines.append(f"def {func_name}(y_true, y_pred):\n")
                code_lines.append(f"    \"\"\"Student-t åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (df={df:.2f}, Î¼={loc:.4f}, Ïƒ={scale:.4f})\"\"\"\n")
                code_lines.append(f"    residual = y_true - y_pred\n")
                code_lines.append(f"    standardized = (residual - {loc:.6f}) / {scale:.6f}\n")
                code_lines.append(f"    nll = -tf.math.lgamma({(df + 1) / 2:.6f}) + tf.math.lgamma({df / 2:.6f}) + \\\n")
                code_lines.append(f"          0.5 * tf.math.log({df * np.pi * scale**2:.6f}) + \\\n")
                code_lines.append(f"          {(df + 1) / 2:.6f} * tf. math.log(1 + tf.square(standardized) / {df:.6f})\n")
                code_lines.append(f"    return tf.reduce_mean(nll)\n\n")

            elif best_dist_name == 'bernoulli':
                p_mean = params[0] if params else 0.5 
                
                code_lines.append(f"def {func_name}(y_true, y_pred):\n")
                code_lines.append(f"    \"\"\"ä¼¯åŠªåˆ©åˆ†å¸ƒè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (å³ Binary Cross Entropy, Avg p={p_mean:.4f})\"\"\"\n")
                code_lines.append(f"    # æ³¨æ„ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒä¸è®¡ç®—æ®‹å·®ï¼Œè€Œæ˜¯ç›´æ¥å¯¹æ¦‚ç‡å»ºæ¨¡\n")
                code_lines.append(f"    # æ•°å€¼ç¨³å®šæ€§å¤„ç†ï¼Œé˜²æ­¢ log(0)\n")
                code_lines.append(f"    epsilon = 1e-7\n")
                code_lines.append(f"    y_pred_c = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n")
                code_lines.append(f"    nll = -(y_true * tf.math.log(y_pred_c) + (1 - y_true) * tf.math.log(1 - y_pred_c))\n")
                code_lines.append(f"    return tf.reduce_mean(nll)\n\n")
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.writelines(code_lines)
        
        print(f"âœ… æŸå¤±å‡½æ•°ä»£ç å·²ä¿å­˜åˆ° {output_path}")

# ============================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»æµç¨‹
# ============================================

def analyze_residual_distribution(
    model: tf.keras.Model,
    dataset: tf.data.Dataset,
    output_dir: str = './residual_analysis',
    num_batches: int = 100
):
    """
    å®Œæ•´çš„æ®‹å·®åˆ†å¸ƒåˆ†ææµç¨‹
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        dataset: éªŒè¯æ•°æ®é›†
        output_dir: è¾“å‡ºç›®å½•
        num_batches: ä½¿ç”¨å¤šå°‘ä¸ªbatchè¿›è¡Œåˆ†æ
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("æ®‹å·®åˆ†å¸ƒåˆ†ææµç¨‹")
    print("="*80)
    
    # æ­¥éª¤ 1: æå–æ®‹å·®
    print("\nğŸ“Š æ­¥éª¤ 1/4: æå–æ®‹å·®...")
    extractor = ResidualExtractor(model, dataset)
    residuals = extractor.extract(num_batches=num_batches)
    
    for key, res in residuals.items():
        print(f"  {key}: {len(res)} ä¸ªæ ·æœ¬, å‡å€¼={res.mean():.4f}, æ ‡å‡†å·®={res.std():.4f}")
    
    # æ­¥éª¤ 2: æ‹Ÿåˆå¤šç§åˆ†å¸ƒ
    print("\nğŸ”¬ æ­¥éª¤ 2/4: æ‹Ÿåˆåˆ†å¸ƒ...")
    fitted_distributions = {}
    
    for res_key, res_data in residuals.items():
        print(f"\n  åˆ†æ {res_key}...")
        results = {}
        
        for dist_name in DistributionFitter.DISTRIBUTIONS. keys():
            params, aic, bic = DistributionFitter.fit_distribution(res_data, dist_name)
            
            if params is None:
                continue
            
            # æ‹Ÿåˆä¼˜åº¦æ£€éªŒ
            gof_tests = DistributionFitter. goodness_of_fit_tests(res_data, dist_name, params)
            
            results[dist_name] = {
                'params': params,
                'aic': aic,
                'bic': bic,
                **gof_tests
            }
            
            print(f"    {dist_name:<15s}:  AIC={aic: 10.2f}, BIC={bic:10.2f}, KS_p={gof_tests['ks_p_value']:.4f}")
        
        # æŒ‰ AIC æ’åº
        ranked = sorted(results.items(), key=lambda x: x[1]['aic'])
        fitted_distributions[res_key] = {
            'ranked_distributions': ranked,
            'best_distribution': ranked[0][0] if ranked else None
        }
    
    # æ­¥éª¤ 3: å¯è§†åŒ–
    print("\nğŸ“ˆ æ­¥éª¤ 3/4: ç”Ÿæˆå¯è§†åŒ–...")
    ResidualVisualizer.plot_residual_analysis(residuals, fitted_distributions, output_dir)
    
    # æ­¥éª¤ 4: ç”ŸæˆæŠ¥å‘Šå’Œä»£ç 
    print("\nğŸ“ æ­¥éª¤ 4/4: ç”ŸæˆæŠ¥å‘Š...")
    ResidualVisualizer. generate_summary_report(
        fitted_distributions, 
        f'{output_dir}/residual_analysis_report.txt'
    )
    
    CustomLossGenerator.save_loss_functions_as_code(
        fitted_distributions,
        f'{output_dir}/custom_loss_functions.py'
    )
    
    # ä¿å­˜æ‹Ÿåˆç»“æœä¸º JSON
    summary = {}
    for res_key, dist_results in fitted_distributions.items():
        summary[res_key] = {
            'best_distribution': dist_results['best_distribution'],
            'top_3_distributions': [
                {
                    'name': name,
                    'aic': info['aic'],
                    'bic': info['bic'],
                    'ks_p_value': info['ks_p_value']
                }
                for name, info in dist_results['ranked_distributions'][: 3]
            ]
        }
    
    with open(f'{output_dir}/fitted_distributions.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨:  {output_dir}/")
    print(f"   - å¯è§†åŒ–å›¾è¡¨: {output_dir}/*_analysis.png")
    print(f"   - æ–‡æœ¬æŠ¥å‘Š: {output_dir}/residual_analysis_report.txt")
    print(f"   - è‡ªå®šä¹‰æŸå¤±å‡½æ•°:  {output_dir}/custom_loss_functions.py")
    print(f"   - JSONæ‘˜è¦: {output_dir}/fitted_distributions.json")
    
    return fitted_distributions, residuals

# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šåŠ è½½æ¨¡å‹å’Œæ•°æ®
    # model = tf.keras.models.load_model('path/to/your/model')
    # dataset = ...   # ä½ çš„ tf.data.Dataset
    
    # analyze_residual_distribution(model, dataset, num_batches=200)
    
    print("è¯·åœ¨ä¸»è„šæœ¬ä¸­å¯¼å…¥æ­¤æ¨¡å—å¹¶è°ƒç”¨ analyze_residual_distribution()")